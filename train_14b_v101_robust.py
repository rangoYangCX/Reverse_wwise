# =============================================================================
# ğŸ”¥ V10.1 - Qwen2.5-Coder-14B (å¥å£®ç‰ˆ - ç¡®ä¿ä¸Šä¼ æˆåŠŸ)
# =============================================================================
# ä¿®å¤:
# 1. LoRA å…ˆä¿å­˜æœ¬åœ°å†ä¸Šä¼ 
# 2. GGUF è½¬æ¢å‰æ¸…ç†æ˜¾å­˜
# 3. ä¸Šä¼ å¸¦éªŒè¯å’Œé‡è¯•
# 4. è¯¦ç»†é”™è¯¯æ—¥å¿—
# =============================================================================

import os
import sys

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import subprocess
subprocess.run("pip install psutil -q", shell=True, capture_output=True)
import psutil
import builtins
builtins.psutil = psutil

import json
import gc
import math
import time
from datetime import timedelta
import torch

gc.collect()
torch.cuda.empty_cache()

from unsloth import FastLanguageModel
from unsloth import is_bfloat16_supported
from trl import SFTTrainer
from transformers import TrainingArguments
from huggingface_hub import hf_hub_download, HfApi, upload_folder
from google.colab import userdata
from datasets import Dataset

print("="*60)
print("ğŸ”¥ V10.1 - Qwen2.5-Coder-14B (å¥å£®ç‰ˆ)")
print("="*60)

# GPU
gpu_name = torch.cuda.get_device_name(0)
total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
print(f"ğŸ® GPU: {gpu_name} ({total_vram:.1f} GB)")

if total_vram < 35:
    print("âš ï¸ 14B æ¨¡å‹éœ€è¦ 40GB æ˜¾å­˜")
    sys.exit(1)

# =============================================================================
# é…ç½®
# =============================================================================
BASE_MODEL = "Qwen/Qwen2.5-Coder-14B-Instruct"

BATCH_SIZE = 2
GRAD_ACCUM = 8
MAX_SEQ = 1024
LORA_R = 64
LORA_ALPHA = 128
TARGET_EPOCHS = 3

print(f"\nâš™ï¸ 14B é…ç½®:")
print(f"   æ¨¡å‹: {BASE_MODEL}")
print(f"   Batch: {BATCH_SIZE} Ã— {GRAD_ACCUM} = {BATCH_SIZE * GRAD_ACCUM}")

# Token
try:
    HF_TOKEN = userdata.get('HF_TOKEN')
    print("   Token: âœ“ (ä» Secrets è¯»å–)")
except:
    HF_TOKEN = input("HF Token: ").strip()
    if not HF_TOKEN: sys.exit("âŒ")

os.environ["HF_TOKEN"] = HF_TOKEN
HF_MODEL_REPO = "chengxuanyyy/Wwise-Engineering-Brain"

# éªŒè¯ Token
print("\nğŸ”‘ éªŒè¯ HuggingFace Token...")
api = HfApi(token=HF_TOKEN)
try:
    user_info = api.whoami()
    print(f"   âœ“ ç™»å½•ç”¨æˆ·: {user_info['name']}")
except Exception as e:
    print(f"   âŒ Token æ— æ•ˆ: {e}")
    sys.exit(1)

# æ•°æ®
print("\nğŸ“‚ æ•°æ®åŠ è½½")
data_path = hf_hub_download(
    repo_id="chengxuanyyy/wwise-dsl-training-data",
    filename="optimized_dataset_processed_processed.jsonl",
    repo_type="dataset", token=HF_TOKEN
)

samples = []
with open(data_path, 'r', encoding='utf-8') as f:
    for line in f:
        if line.strip():
            try: samples.append(json.loads(line))
            except: pass

TOTAL = len(samples)
EFF_BATCH = BATCH_SIZE * GRAD_ACCUM
STEPS = math.ceil(TOTAL / EFF_BATCH) * TARGET_EPOCHS
print(f"âœ… æ ·æœ¬: {TOTAL}, æ­¥æ•°: {STEPS}")

# =============================================================================
# æ¨¡å‹
# =============================================================================
print("\nğŸ¤– åŠ è½½ 14B æ¨¡å‹...")

gc.collect()
torch.cuda.empty_cache()

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=BASE_MODEL,
    max_seq_length=MAX_SEQ,
    dtype=None,
    load_in_4bit=True,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=LORA_R,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=LORA_ALPHA,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=42,
)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# =============================================================================
# æ•°æ®æ ¼å¼åŒ–
# =============================================================================
print("\nğŸ“ æ•°æ®æ ¼å¼åŒ–")

TEMPLATE = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

formatted = [{"text": TEMPLATE.format(s.get("instruction",""), s.get("input",""), s.get("output","")) + tokenizer.eos_token} for s in samples]
dataset = Dataset.from_list(formatted)

# =============================================================================
# è®­ç»ƒ
# =============================================================================
print("\nâš”ï¸ è®­ç»ƒ")

training_args = TrainingArguments(
    output_dir="outputs",
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM,
    warmup_steps=50,
    num_train_epochs=TARGET_EPOCHS,
    learning_rate=1e-4,
    fp16=not is_bfloat16_supported(),
    bf16=is_bfloat16_supported(),
    logging_steps=20,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="cosine",
    seed=42,
    save_strategy="steps",
    save_steps=500,
    save_total_limit=2,
    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ,
    dataset_num_proc=4,
    packing=False,
    args=training_args,
)

print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ! Steps: {STEPS}")

start = time.time()
trainer.train()
train_time = str(timedelta(seconds=int(time.time() - start)))

print(f"\nâœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {train_time}")

# =============================================================================
# ğŸ’¾ Step 1: ä¿å­˜ LoRA åˆ°æœ¬åœ°
# =============================================================================
print("\n" + "="*60)
print("ğŸ’¾ Step 1: ä¿å­˜ LoRA Adapter")
print("="*60)

LORA_DIR = "lora_adapter"

# æ¸…ç†
del trainer
gc.collect()
torch.cuda.empty_cache()

# ä¿å­˜ LoRA
print("ä¿å­˜ LoRA adapter åˆ°æœ¬åœ°...")
model.save_pretrained(LORA_DIR)
tokenizer.save_pretrained(LORA_DIR)

# éªŒè¯æ–‡ä»¶
lora_files = os.listdir(LORA_DIR)
print(f"âœ“ ä¿å­˜äº† {len(lora_files)} ä¸ªæ–‡ä»¶:")
for f in lora_files:
    size = os.path.getsize(os.path.join(LORA_DIR, f)) / 1024 / 1024
    print(f"   {f}: {size:.1f} MB")

# æ£€æŸ¥å…³é”®æ–‡ä»¶
required_files = ["adapter_config.json", "adapter_model.safetensors"]
missing = [f for f in required_files if f not in lora_files]
if missing:
    print(f"âš ï¸ ç¼ºå°‘æ–‡ä»¶: {missing}")
else:
    print("âœ“ å…³é”®æ–‡ä»¶å®Œæ•´")

# =============================================================================
# ğŸ’¾ Step 2: ä¸Šä¼  LoRA åˆ° HuggingFace
# =============================================================================
print("\n" + "="*60)
print("ğŸ’¾ Step 2: ä¸Šä¼  LoRA åˆ° HuggingFace")
print("="*60)

MAX_RETRIES = 3

for attempt in range(MAX_RETRIES):
    try:
        print(f"å°è¯• {attempt + 1}/{MAX_RETRIES}...")
        
        # ä½¿ç”¨ upload_folder ä¸Šä¼ æ•´ä¸ªç›®å½•
        api.upload_folder(
            folder_path=LORA_DIR,
            repo_id=HF_MODEL_REPO,
            commit_message=f"14B LoRA V10.1 ({train_time})",
            token=HF_TOKEN,
        )
        
        print("âœ… LoRA ä¸Šä¼ æˆåŠŸ!")
        break
        
    except Exception as e:
        print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
        if attempt < MAX_RETRIES - 1:
            print("ç­‰å¾… 10 ç§’åé‡è¯•...")
            time.sleep(10)
        else:
            print("âš ï¸ LoRA ä¸Šä¼ å¤±è´¥,ä½†ç»§ç»­æ‰§è¡Œ GGUF è½¬æ¢")

# =============================================================================
# ğŸ“¦ Step 3: GGUF è½¬æ¢
# =============================================================================
print("\n" + "="*60)
print("ğŸ“¦ Step 3: GGUF è½¬æ¢ (14B éœ€è¦è¾ƒé•¿æ—¶é—´)")
print("="*60)

GGUF_DIR = "model_gguf"

# æ¸…ç†æ˜¾å­˜ç»™ GGUF è½¬æ¢
print("æ¸…ç†æ˜¾å­˜...")
gc.collect()
torch.cuda.empty_cache()

used = torch.cuda.memory_allocated(0) / 1024**3
print(f"å½“å‰æ˜¾å­˜: {used:.1f} GB")

# GGUF è½¬æ¢
gguf_success = False
try:
    print("\nå¼€å§‹ GGUF è½¬æ¢ (q4_k_m)...")
    print("â³ è¿™å¯èƒ½éœ€è¦ 10-20 åˆ†é’Ÿ,è¯·è€å¿ƒç­‰å¾…...")
    
    model.save_pretrained_gguf(
        GGUF_DIR, 
        tokenizer, 
        quantization_method="q4_k_m"
    )
    
    # éªŒè¯ GGUF æ–‡ä»¶
    if os.path.exists(GGUF_DIR):
        gguf_files = [f for f in os.listdir(GGUF_DIR) if f.endswith(".gguf")]
        if gguf_files:
            gguf_file = gguf_files[0]
            gguf_path = os.path.join(GGUF_DIR, gguf_file)
            gguf_size = os.path.getsize(gguf_path) / 1024 / 1024 / 1024
            print(f"âœ… GGUF è½¬æ¢æˆåŠŸ!")
            print(f"   æ–‡ä»¶: {gguf_file}")
            print(f"   å¤§å°: {gguf_size:.2f} GB")
            gguf_success = True
        else:
            print("âŒ GGUF ç›®å½•å­˜åœ¨ä½†æ²¡æœ‰ .gguf æ–‡ä»¶")
    else:
        print("âŒ GGUF ç›®å½•ä¸å­˜åœ¨")
        
except Exception as e:
    print(f"âŒ GGUF è½¬æ¢å¤±è´¥: {e}")
    print("\nğŸ’¡ æç¤º: å¯ä»¥ç¨åæ‰‹åŠ¨è½¬æ¢")
    print("   1. ä¸‹è½½ LoRA: git clone https://huggingface.co/" + HF_MODEL_REPO)
    print("   2. åˆå¹¶æ¨¡å‹: python merge_lora.py")
    print("   3. è½¬æ¢ GGUF: python llama.cpp/convert.py")

# =============================================================================
# ğŸ“¤ Step 4: ä¸Šä¼  GGUF
# =============================================================================
if gguf_success:
    print("\n" + "="*60)
    print("ğŸ“¤ Step 4: ä¸Šä¼  GGUF åˆ° HuggingFace")
    print("="*60)
    
    gguf_path = os.path.join(GGUF_DIR, gguf_file)
    remote_path = f"gguf/wwise-brain-14b-q4_k_m.gguf"
    
    print(f"ä¸Šä¼  {gguf_size:.2f} GB æ–‡ä»¶...")
    print("â³ å¤§æ–‡ä»¶ä¸Šä¼ å¯èƒ½éœ€è¦ 10-30 åˆ†é’Ÿ...")
    
    for attempt in range(MAX_RETRIES):
        try:
            print(f"å°è¯• {attempt + 1}/{MAX_RETRIES}...")
            
            api.upload_file(
                path_or_fileobj=gguf_path,
                path_in_repo=remote_path,
                repo_id=HF_MODEL_REPO,
                token=HF_TOKEN,
            )
            
            print("âœ… GGUF ä¸Šä¼ æˆåŠŸ!")
            break
            
        except Exception as e:
            print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
            if attempt < MAX_RETRIES - 1:
                print("ç­‰å¾… 30 ç§’åé‡è¯•...")
                time.sleep(30)
            else:
                print("âš ï¸ GGUF ä¸Šä¼ å¤±è´¥")
                print(f"   æœ¬åœ°æ–‡ä»¶ä¿å­˜åœ¨: {gguf_path}")
                print("   ä½ å¯ä»¥æ‰‹åŠ¨ä¸Šä¼ åˆ° HuggingFace")
    
    # ä¸Šä¼  Modelfile
    print("\nä¸Šä¼  Modelfile...")
    try:
        modelfile_content = '''FROM ./wwise-brain-14b-q4_k_m.gguf

TEMPLATE """<|im_start|>system
{{ .System }}<|im_end|>
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""

SYSTEM """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Wwise éŸ³é¢‘æŠ€æœ¯ä¸“å®¶,ç²¾é€š DSL ä»£ç ç”Ÿæˆã€‚æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚,ç”Ÿæˆç¬¦åˆ Wwise å·¥ç¨‹è§„èŒƒçš„ DSL ä»£ç ã€‚"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
'''
        
        modelfile_path = "Modelfile"
        with open(modelfile_path, "w") as f:
            f.write(modelfile_content)
        
        api.upload_file(
            path_or_fileobj=modelfile_path,
            path_in_repo="gguf/Modelfile",
            repo_id=HF_MODEL_REPO,
            token=HF_TOKEN,
        )
        print("âœ… Modelfile ä¸Šä¼ æˆåŠŸ!")
        
    except Exception as e:
        print(f"âš ï¸ Modelfile ä¸Šä¼ å¤±è´¥: {e}")

# =============================================================================
# âœ… æœ€ç»ˆéªŒè¯
# =============================================================================
print("\n" + "="*60)
print("âœ… æœ€ç»ˆéªŒè¯")
print("="*60)

try:
    from huggingface_hub import list_repo_files
    
    files = list_repo_files(HF_MODEL_REPO, token=HF_TOKEN)
    
    print(f"\nğŸ“¦ {HF_MODEL_REPO} æ–‡ä»¶åˆ—è¡¨:")
    
    # åˆ†ç±»æ˜¾ç¤º
    lora_files_remote = [f for f in files if not f.startswith("gguf/")]
    gguf_files_remote = [f for f in files if f.startswith("gguf/")]
    
    print("\nğŸ”§ LoRA æ–‡ä»¶:")
    for f in lora_files_remote[:10]:
        print(f"   {f}")
    if len(lora_files_remote) > 10:
        print(f"   ... è¿˜æœ‰ {len(lora_files_remote) - 10} ä¸ªæ–‡ä»¶")
    
    print("\nğŸ“¦ GGUF æ–‡ä»¶:")
    for f in gguf_files_remote:
        print(f"   {f}")
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    print("\nğŸ” å…³é”®æ–‡ä»¶æ£€æŸ¥:")
    checks = [
        ("adapter_config.json", "adapter_config.json" in files),
        ("adapter_model.safetensors", "adapter_model.safetensors" in files),
        ("GGUF æ¨¡å‹", any("gguf" in f.lower() and f.endswith(".gguf") for f in files)),
        ("Modelfile", "gguf/Modelfile" in files),
    ]
    
    all_ok = True
    for name, ok in checks:
        status = "âœ…" if ok else "âŒ"
        print(f"   {status} {name}")
        if not ok:
            all_ok = False
    
    if all_ok:
        print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ æˆåŠŸ!")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æ–‡ä»¶ç¼ºå¤±,è¯·æ£€æŸ¥")
        
except Exception as e:
    print(f"âš ï¸ éªŒè¯å¤±è´¥: {e}")

# =============================================================================
# ğŸ‰ å®Œæˆ
# =============================================================================
print("\n" + "="*60)
print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
print("="*60)

print(f"""
ğŸ“Š è®­ç»ƒç»Ÿè®¡:
   æ¨¡å‹: Qwen2.5-Coder-14B
   æ•°æ®: {TOTAL} æ ·æœ¬
   æ­¥æ•°: {STEPS}
   æ—¶é—´: {train_time}

ğŸ“¦ æ¨¡å‹åœ°å€:
   https://huggingface.co/{HF_MODEL_REPO}

ğŸš€ ä½¿ç”¨æ–¹æ³•:

1. Ollama (æœ¬åœ°éƒ¨ç½²):
   # ä¸‹è½½ GGUF
   wget https://huggingface.co/{HF_MODEL_REPO}/resolve/main/gguf/wwise-brain-14b-q4_k_m.gguf
   
   # ä¸‹è½½ Modelfile
   wget https://huggingface.co/{HF_MODEL_REPO}/resolve/main/gguf/Modelfile
   
   # åˆ›å»º Ollama æ¨¡å‹
   ollama create wwise-brain-14b -f Modelfile
   
   # è¿è¡Œ
   ollama run wwise-brain-14b

2. Python (ä½¿ç”¨ LoRA):
   from unsloth import FastLanguageModel
   model, tokenizer = FastLanguageModel.from_pretrained("{HF_MODEL_REPO}")
""")

print("="*60)
