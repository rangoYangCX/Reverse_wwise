# =============================================================================
# ğŸ›‘ æ‰‹åŠ¨ä¸­æ–­å - GGUF æ‰“åŒ… & ä¸Šä¼ è„šæœ¬
# =============================================================================
# ä½¿ç”¨åœºæ™¯:
# 1. è®­ç»ƒå·®ä¸å¤šäº†,æƒ³æå‰åœæ­¢
# 2. è®­ç»ƒè¢«æ„å¤–ä¸­æ–­
# 3. è®­ç»ƒå®Œæˆä½†ä¸Šä¼ å¤±è´¥
#
# ä½¿ç”¨æ–¹æ³•:
# 1. åœæ­¢è®­ç»ƒ (Ctrl+C æˆ–åœæ­¢æŒ‰é’®)
# 2. è¿è¡Œæ­¤è„šæœ¬
# =============================================================================

import os
import sys
import gc
import time
from datetime import datetime

print("="*60)
print("ğŸ›‘ æ‰‹åŠ¨ä¸­æ–­å - GGUF æ‰“åŒ… & ä¸Šä¼ ")
print("="*60)

# =============================================================================
# Step 1: æŸ¥æ‰¾å·²ä¿å­˜çš„æ¨¡å‹
# =============================================================================
print("\nğŸ” Step 1: æŸ¥æ‰¾å·²ä¿å­˜çš„æ¨¡å‹")
print("-"*40)

# å¯èƒ½çš„ä¿å­˜ä½ç½®
possible_dirs = [
    "outputs/checkpoint-3500",
    "outputs/checkpoint-3000",
    "outputs/checkpoint-2500",
    "outputs/checkpoint-2000",
    "outputs/checkpoint-1500",
    "outputs/checkpoint-1000",
    "outputs/checkpoint-500",
    "outputs/final",
    "outputs",
    "lora_adapter",
]

found_checkpoint = None
found_step = 0

for d in possible_dirs:
    if os.path.exists(d):
        # æ£€æŸ¥æ˜¯å¦æœ‰ adapter æ–‡ä»¶
        adapter_config = os.path.join(d, "adapter_config.json")
        adapter_model = os.path.join(d, "adapter_model.safetensors")
        
        if os.path.exists(adapter_config):
            # æå–æ­¥æ•°
            if "checkpoint-" in d:
                step = int(d.split("-")[-1])
            else:
                step = 0
            
            if step > found_step:
                found_step = step
                found_checkpoint = d
            elif found_checkpoint is None:
                found_checkpoint = d

if found_checkpoint:
    print(f"âœ… æ‰¾åˆ°æ¨¡å‹: {found_checkpoint}")
    if found_step > 0:
        print(f"   æ­¥æ•°: {found_step}")
    
    # åˆ—å‡ºæ–‡ä»¶
    files = os.listdir(found_checkpoint)
    print(f"   æ–‡ä»¶æ•°: {len(files)}")
    for f in files[:5]:
        size = os.path.getsize(os.path.join(found_checkpoint, f)) / 1024 / 1024
        print(f"   - {f} ({size:.1f} MB)")
    if len(files) > 5:
        print(f"   ... è¿˜æœ‰ {len(files) - 5} ä¸ªæ–‡ä»¶")
else:
    print("âŒ æœªæ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹!")
    print("\nå¯èƒ½çš„åŸå› :")
    print("1. è®­ç»ƒæœªè¾¾åˆ°ç¬¬ä¸€ä¸ª save_steps (500)")
    print("2. ä¿å­˜ç›®å½•ä¸åŒ")
    print("\nè¯·æ£€æŸ¥ outputs/ ç›®å½•")
    
    if os.path.exists("outputs"):
        print("\noutputs/ ç›®å½•å†…å®¹:")
        for f in os.listdir("outputs"):
            print(f"   {f}")
    
    sys.exit(1)

# =============================================================================
# Step 2: é…ç½®
# =============================================================================
print("\nâš™ï¸ Step 2: é…ç½®")
print("-"*40)

# HuggingFace é…ç½®
from google.colab import userdata

try:
    HF_TOKEN = userdata.get('HF_TOKEN')
    print("âœ“ Token: ä» Secrets è¯»å–")
except:
    HF_TOKEN = input("è¯·è¾“å…¥ HF Token: ").strip()
    if not HF_TOKEN:
        sys.exit("âŒ Token ä¸èƒ½ä¸ºç©º")

HF_MODEL_REPO = "chengxuanyyy/Wwise-Engineering-Brain"

# æ£€æµ‹æ¨¡å‹å¤§å° (7B or 14B)
adapter_config_path = os.path.join(found_checkpoint, "adapter_config.json")
import json
with open(adapter_config_path, 'r') as f:
    config = json.load(f)

base_model = config.get("base_model_name_or_path", "")
if "14B" in base_model or "14b" in base_model:
    MODEL_SIZE = "14B"
    BASE_MODEL = "Qwen/Qwen2.5-Coder-14B-Instruct"
else:
    MODEL_SIZE = "7B"
    BASE_MODEL = "Qwen/Qwen2.5-Coder-7B-Instruct"

print(f"âœ“ åŸºåº§æ¨¡å‹: {BASE_MODEL}")
print(f"âœ“ æ¨¡å‹å¤§å°: {MODEL_SIZE}")
print(f"âœ“ ç›®æ ‡ä»“åº“: {HF_MODEL_REPO}")

# =============================================================================
# Step 3: éªŒè¯ HuggingFace Token
# =============================================================================
print("\nğŸ”‘ Step 3: éªŒè¯ Token")
print("-"*40)

from huggingface_hub import HfApi, list_repo_files

api = HfApi(token=HF_TOKEN)

try:
    user_info = api.whoami()
    print(f"âœ“ ç™»å½•ç”¨æˆ·: {user_info['name']}")
except Exception as e:
    print(f"âŒ Token æ— æ•ˆ: {e}")
    sys.exit(1)

# =============================================================================
# Step 4: ä¸Šä¼  LoRA Adapter
# =============================================================================
print("\nğŸ“¤ Step 4: ä¸Šä¼  LoRA Adapter")
print("-"*40)

MAX_RETRIES = 3

for attempt in range(MAX_RETRIES):
    try:
        print(f"å°è¯• {attempt + 1}/{MAX_RETRIES}...")
        
        api.upload_folder(
            folder_path=found_checkpoint,
            repo_id=HF_MODEL_REPO,
            commit_message=f"LoRA {MODEL_SIZE} (step {found_step})",
            token=HF_TOKEN,
        )
        
        print("âœ… LoRA ä¸Šä¼ æˆåŠŸ!")
        break
        
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")
        if attempt < MAX_RETRIES - 1:
            print("ç­‰å¾… 10 ç§’åé‡è¯•...")
            time.sleep(10)
        else:
            print("âš ï¸ LoRA ä¸Šä¼ å¤±è´¥,ç»§ç»­å°è¯• GGUF...")

# =============================================================================
# Step 5: åŠ è½½æ¨¡å‹ç”¨äº GGUF è½¬æ¢
# =============================================================================
print("\nğŸ¤– Step 5: åŠ è½½æ¨¡å‹")
print("-"*40)

import subprocess
subprocess.run("pip install psutil -q", shell=True, capture_output=True)
import psutil
import builtins
builtins.psutil = psutil

import torch
gc.collect()
torch.cuda.empty_cache()

print(f"åŠ è½½ {MODEL_SIZE} æ¨¡å‹...")
print("â³ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")

from unsloth import FastLanguageModel

# å…ˆåŠ è½½åŸºåº§æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=BASE_MODEL,
    max_seq_length=1024,
    dtype=None,
    load_in_4bit=True,
)

# åŠ è½½ LoRA adapter
from peft import PeftModel
print(f"åŠ è½½ LoRA adapter: {found_checkpoint}")
model = PeftModel.from_pretrained(model, found_checkpoint)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

gc.collect()
torch.cuda.empty_cache()

# =============================================================================
# Step 6: GGUF è½¬æ¢
# =============================================================================
print("\nğŸ“¦ Step 6: GGUF è½¬æ¢")
print("-"*40)

GGUF_DIR = "model_gguf"

print(f"å¼€å§‹ GGUF è½¬æ¢ ({MODEL_SIZE}, q4_k_m)...")
print("â³ è¿™å¯èƒ½éœ€è¦ 10-20 åˆ†é’Ÿ,è¯·è€å¿ƒç­‰å¾…...")

start_time = time.time()

try:
    # åˆå¹¶ LoRA åˆ°åŸºåº§æ¨¡å‹
    print("åˆå¹¶ LoRA weights...")
    model = model.merge_and_unload()
    
    gc.collect()
    torch.cuda.empty_cache()
    
    # GGUF è½¬æ¢
    print("è½¬æ¢ä¸º GGUF...")
    model.save_pretrained_gguf(
        GGUF_DIR,
        tokenizer,
        quantization_method="q4_k_m"
    )
    
    elapsed = time.time() - start_time
    print(f"âœ… GGUF è½¬æ¢å®Œæˆ! è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
    
    # éªŒè¯
    gguf_files = [f for f in os.listdir(GGUF_DIR) if f.endswith(".gguf")]
    if gguf_files:
        gguf_file = gguf_files[0]
        gguf_path = os.path.join(GGUF_DIR, gguf_file)
        gguf_size = os.path.getsize(gguf_path) / 1024**3
        print(f"   æ–‡ä»¶: {gguf_file}")
        print(f"   å¤§å°: {gguf_size:.2f} GB")
    else:
        print("âŒ GGUF æ–‡ä»¶æœªç”Ÿæˆ")
        sys.exit(1)
        
except Exception as e:
    print(f"âŒ GGUF è½¬æ¢å¤±è´¥: {e}")
    print("\nğŸ’¡ å¯ä»¥å°è¯•æ‰‹åŠ¨è½¬æ¢,å‚è€ƒ llama.cpp")
    sys.exit(1)

# =============================================================================
# Step 7: ä¸Šä¼  GGUF
# =============================================================================
print("\nğŸ“¤ Step 7: ä¸Šä¼  GGUF")
print("-"*40)

gguf_path = os.path.join(GGUF_DIR, gguf_file)
remote_name = f"wwise-brain-{MODEL_SIZE.lower()}-q4_k_m.gguf"

print(f"ä¸Šä¼  {gguf_size:.2f} GB æ–‡ä»¶...")
print("â³ å¤§æ–‡ä»¶ä¸Šä¼ å¯èƒ½éœ€è¦ 10-30 åˆ†é’Ÿ...")

for attempt in range(MAX_RETRIES):
    try:
        print(f"å°è¯• {attempt + 1}/{MAX_RETRIES}...")
        
        api.upload_file(
            path_or_fileobj=gguf_path,
            path_in_repo=f"gguf/{remote_name}",
            repo_id=HF_MODEL_REPO,
            token=HF_TOKEN,
        )
        
        print("âœ… GGUF ä¸Šä¼ æˆåŠŸ!")
        break
        
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")
        if attempt < MAX_RETRIES - 1:
            print("ç­‰å¾… 30 ç§’åé‡è¯•...")
            time.sleep(30)
        else:
            print(f"âš ï¸ GGUF ä¸Šä¼ å¤±è´¥,æœ¬åœ°æ–‡ä»¶ä¿å­˜åœ¨: {gguf_path}")

# =============================================================================
# Step 8: ä¸Šä¼  Modelfile
# =============================================================================
print("\nğŸ“„ Step 8: ä¸Šä¼  Modelfile")
print("-"*40)

modelfile_content = f'''FROM ./{remote_name}

TEMPLATE """<|im_start|>system
{{{{ .System }}}}<|im_end|>
<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
<|im_start|>assistant
"""

SYSTEM """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Wwise éŸ³é¢‘æŠ€æœ¯ä¸“å®¶,ç²¾é€š DSL ä»£ç ç”Ÿæˆã€‚æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚,ç”Ÿæˆç¬¦åˆ Wwise å·¥ç¨‹è§„èŒƒçš„ DSL ä»£ç ã€‚"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
'''

try:
    with open("Modelfile", "w") as f:
        f.write(modelfile_content)
    
    api.upload_file(
        path_or_fileobj="Modelfile",
        path_in_repo="gguf/Modelfile",
        repo_id=HF_MODEL_REPO,
        token=HF_TOKEN,
    )
    print("âœ… Modelfile ä¸Šä¼ æˆåŠŸ!")
    
except Exception as e:
    print(f"âš ï¸ Modelfile ä¸Šä¼ å¤±è´¥: {e}")

# =============================================================================
# Step 9: æœ€ç»ˆéªŒè¯
# =============================================================================
print("\nâœ… Step 9: æœ€ç»ˆéªŒè¯")
print("-"*40)

try:
    files = list_repo_files(HF_MODEL_REPO, token=HF_TOKEN)
    
    checks = [
        ("adapter_config.json", "adapter_config.json" in files),
        ("adapter_model.safetensors", "adapter_model.safetensors" in files),
        (f"gguf/{remote_name}", f"gguf/{remote_name}" in files),
        ("gguf/Modelfile", "gguf/Modelfile" in files),
    ]
    
    print("\nğŸ“‹ æ–‡ä»¶æ£€æŸ¥:")
    all_ok = True
    for name, ok in checks:
        status = "âœ…" if ok else "âŒ"
        print(f"   {status} {name}")
        if not ok:
            all_ok = False
    
    if all_ok:
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ æˆåŠŸ!")
        print("="*60)
    else:
        print("\nâš ï¸ éƒ¨åˆ†æ–‡ä»¶ç¼ºå¤±")
        
except Exception as e:
    print(f"âŒ éªŒè¯å¤±è´¥: {e}")

# =============================================================================
# å®Œæˆ
# =============================================================================
print(f"""
{'='*60}
ğŸ“¦ æ¨¡å‹åœ°å€: https://huggingface.co/{HF_MODEL_REPO}

ğŸš€ æœ¬åœ°éƒ¨ç½² (Ollama):

   # 1. ä¸‹è½½æ–‡ä»¶
   wget https://huggingface.co/{HF_MODEL_REPO}/resolve/main/gguf/{remote_name}
   wget https://huggingface.co/{HF_MODEL_REPO}/resolve/main/gguf/Modelfile
   
   # 2. åˆ›å»ºæ¨¡å‹
   ollama create wwise-brain -f Modelfile
   
   # 3. è¿è¡Œ
   ollama run wwise-brain

{'='*60}
""")
