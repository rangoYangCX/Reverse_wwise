# -*- coding: utf-8 -*-
"""
MMO Engineering Brain - Ultimate One-Click Script
é›†æˆï¼šæ•°æ®æ¸…æ´—å·¥å‚ + èµ„æ·±è®¾è®¡å¸ˆé‡å†™ + æ¨¡å‹è®­ç»ƒ + HFè‡ªåŠ¨äº¤ä»˜
"""

import os
import sys
import subprocess
import json
import torch
import shutil
import gc
import random
import re
import unicodedata
import numpy as np

# --- ğŸ› ï¸ 0. æ ¸å¿ƒä¿®å¤ï¼šæ³¨å…¥ psutil (å¿…é¡»åœ¨æ‰€æœ‰ import ä¹‹å‰) ---
try:
    import psutil
    import builtins
    builtins.psutil = psutil
    print("âœ… ç¯å¢ƒè‡ªæ£€ï¼špsutil æ³¨å…¥æˆåŠŸã€‚")
except ImportError:
    print("âš ï¸ ç¯å¢ƒè‡ªæ£€ï¼šæ­£åœ¨é¢„è£… psutil...")
    subprocess.check_call("pip install psutil", shell=True)
    import psutil
    import builtins
    builtins.psutil = psutil

def clean_memory():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

# --- 1. ç”¨æˆ·é…ç½®åŒº ---
RAW_DATA_FILE = "wwise_training_data_v7.jsonl" # ä½ çš„åŸå§‹ä¸Šä¼ æ–‡ä»¶
CLEAN_DATA_FILE = "final_train_data.jsonl"    # æ¸…æ´—ç”Ÿæˆçš„ç›®æ ‡æ–‡ä»¶
HF_TOKEN = "HF_TOKEN"
HF_REPO_NAME = "chengxuanyyy/Wwise-Engineering-Brain"
QUANTIZATION_METHOD = "q4_k_m"

# --- 2. æ•°æ®å·¥å‚ï¼šæ¸…æ´—ä¸é‡å†™æ¨¡å— ---
# (è¿™é‡Œé›†æˆäº†ä¹‹å‰çš„ generate_action_data.py é€»è¾‘)

DESIGNER_PHRASES = {
    "footstep_system_setup": [
        "æ­å»ºä¸€å¥—æ ‡å‡†çš„ä¸»è§’è„šæ­¥å£°é€»è¾‘ï¼Œæ ¹èŠ‚ç‚¹å« {name}ï¼Œè®°å¾—æŠŠå®šä½(Positioning)è¦†ç›–æ‰“å¼€ã€‚",
        "ç»™ä¸»è§’æ•´ä¸€å¥—è„šæ­¥å£°æ¶æ„ {name}ï¼ŒæŒ‚åœ¨ {parent} ä¸‹é¢ã€‚è¡°å‡è¦è®¾å¥½ï¼Œèµ° OutputBus è·¯ç”±ã€‚",
        "åˆå§‹åŒ–ä¸»è§’çš„è„šæ­¥ç³»ç»Ÿ {name}ã€‚æŠ€æœ¯è¦æ±‚ï¼šå¼€å¯ OverridePositioningï¼Œå¹¶å…³è”åˆ°æè´¨ Switch Groupã€‚"
    ],
    "footstep_material_switch": [
        "ç°åœ¨å¤„ç† {material} æè´¨çš„è„šæ­¥å£°é€»è¾‘ã€‚åœ¨ {parent} ä¸‹å»ºä¸ª SwitchContainer å« {name}ã€‚",
        "æ–°å¢ä¸€ç§åœ°è¡¨æè´¨ï¼š{material}ã€‚åˆ›å»ºå¯¹åº”çš„å®¹å™¨ {name}ï¼Œåˆ«å¿˜äº†æŠŠ Switch Group è¿ä¸Šã€‚",
        "é…ç½® {material} æè´¨çš„ Switch é€»è¾‘ï¼Œå®¹å™¨å‘½åä¸º {name}ï¼Œä¿¡å·èµ° HostPlayerSkill æ€»çº¿ã€‚"
    ],
    "footstep_sfx_assets": [
        "å¯¼å…¥ä¸€æ‰¹ {material} çš„è„šæ­¥å£°ç´ æ {name}ï¼Œè¦é‚£ç§{adjective}çš„æ„Ÿè§‰ã€‚",
        "å¡«å…… {parent} å®¹å™¨çš„å†…å®¹ï¼Œåˆ›å»ºä¸€ç»„éšæœºè„šæ­¥å£° {name}ã€‚å¬æ„Ÿè¦{adjective}ä¸€ç‚¹ã€‚",
        "æŠŠç¾æœ¯ç»™çš„ {material} è„šæ­¥å£° {name} å¯¼è¿›å»ï¼Œæ”¾åˆ° {parent} ä¸‹é¢ï¼Œåšæˆ Random å®¹å™¨ã€‚"
    ],
    "default": [
        "åˆ›å»º {type} å¯¹è±¡ {name}ï¼Œçˆ¶çº§æ˜¯ {parent}ã€‚",
        "åœ¨ {parent} èŠ‚ç‚¹ä¸‹æ–°å¢ {name}ã€‚",
        "å®ç° {name} çš„é€»è¾‘é…ç½®ï¼Œç±»å‹ä¸º {type}ã€‚"
    ]
    # ... (ä¸ºäº†è„šæœ¬ç®€æ´ï¼Œè¿™é‡Œä¿ç•™æ ¸å¿ƒè¯æœ¯ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä¸¾ä¸€åä¸‰)
}
ADJECTIVES = ["æ¹¿æ¼‰æ¼‰", "æ¸…è„†", "åšé‡", "æ²‰é—·", "å°–é”", "æœ‰å¼¹æ€§", "æ‹–æ²“", "åˆ©è½", "å¸¦é‡‘å±æ„Ÿ"]

def nuclear_clean_text(text):
    """æ ¸å¼¹çº§æ¸…æ´—ï¼šå»é™¤å…¨è§’å­—ç¬¦å’Œä¹±ç """
    if not text: return ""
    char_map = {
        'â€œ': '"', 'â€': '"', 'â€˜': "'", 'â€™': "'", 'ï¼š': ':', 'ï¼ˆ': '(', 'ï¼‰': ')', 
        'ï¼Œ': ',', 'ï¼›': ';', 'ã€€': ' ', 'ã€‚': '.', 'ã€': ',', 'ï¼Ÿ': '?', 'ï¼': '!', 'ã€': '[', 'ã€‘': ']'
    }
    for k, v in char_map.items(): text = text.replace(k, v)
    text = unicodedata.normalize('NFKC', text)
    pattern = re.compile(r'[^\u0009\u000A\u000D\u0020-\u007E\u4E00-\u9FFF]')
    return pattern.sub('', text).strip()

def analyze_wwise_code(code_str):
    """ç®€åŒ–çš„æ„å›¾åˆ†æå™¨"""
    lines = code_str.split('\n')
    first_line = lines[0]
    match = re.search(r'CREATE\s+(\w+)\s+"([^"]+)"\s+UNDER\s+"([^"]+)"', first_line)
    params = {}
    if match:
        obj_type, obj_name, parent_name = match.groups()
        params = {"type": obj_type, "name": obj_name, "parent": parent_name, "adjective": random.choice(ADJECTIVES)}
    else:
        return "default", {"name": "Unknown", "type": "Unknown", "parent": "Unknown"}
    
    name_lower = params["name"].lower()
    if "footstep" in name_lower:
        if params["type"] == "ActorMixer": return "footstep_system_setup", params
        if params["type"] == "SwitchContainer": 
            params["material"] = "æŸç§"
            return "footstep_material_switch", params
        if params["type"] in ["Sound", "RandomSequenceContainer"]: 
            params["material"] = "é€šç”¨"
            return "footstep_sfx_assets", params
    return "default", params

def prepare_data():
    print(f"\nğŸ­ å¯åŠ¨æ•°æ®å·¥å‚ï¼šæ­£åœ¨æ¸…æ´—å¹¶é‡å†™ {RAW_DATA_FILE}...")
    if not os.path.exists(RAW_DATA_FILE):
        sys.exit(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° {RAW_DATA_FILE}ï¼Œè¯·å…ˆä¸Šä¼ ï¼")
    
    count = 0
    with open(CLEAN_DATA_FILE, 'w', encoding='utf-8') as outfile:
        with open(RAW_DATA_FILE, 'r', encoding='utf-8', errors='ignore') as infile:
            for line in infile:
                line = nuclear_clean_text(line)
                if not line: continue
                try:
                    data = json.loads(line)
                    code_output = data.get("output", "")
                    if not code_output: continue
                    
                    # ç”Ÿæˆèµ„æ·±è¯æœ¯
                    intent, params = analyze_wwise_code(code_output)
                    templates = DESIGNER_PHRASES.get(intent, DESIGNER_PHRASES["default"])
                    new_instruction = random.choice(templates).format(**params)
                    
                    # å†æ¬¡æ¸…æ´—ç”Ÿæˆçš„å†…å®¹ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
                    data["instruction"] = nuclear_clean_text(new_instruction)
                    data["input"] = nuclear_clean_text(f"å·¥ç¨‹ä¸Šä¸‹æ–‡: {intent} | å¯¹è±¡: {data.get('meta', {}).get('root_type', 'Object')}")
                    
                    outfile.write(json.dumps(data, ensure_ascii=False) + "\n")
                    count += 1
                except Exception:
                    continue
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼å·²ç”Ÿæˆ {count} æ¡é«˜è´¨é‡è®­ç»ƒæ•°æ® -> {CLEAN_DATA_FILE}")

# --- 3. ç¯å¢ƒä¸è®­ç»ƒé…ç½® ---
print("="*50)
print("ğŸš€ å¯åŠ¨ MMO å·¥ç¨‹å¤§è„‘ - ç»ˆæè®­ç»ƒæµæ°´çº¿")
print("="*50)

# æ‰§è¡Œæ•°æ®å‡†å¤‡
prepare_data()
clean_memory()

# å®‰è£…ä¾èµ–
def install_package(command):
    try:
        subprocess.check_call(command, shell=True)
    except Exception:
        pass

print("\nğŸ“¦ æ­£åœ¨é…ç½®è®­ç»ƒç¯å¢ƒ...")
install_package("pip install psutil huggingface_hub")
install_package("pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"")
install_package("pip install unsloth_zoo") # æ˜¾å¼å®‰è£… zoo é˜²æ­¢æŠ¥é”™
install_package("pip install --no-deps xformers trl peft accelerate bitsandbytes")

try:
    from unsloth import FastLanguageModel
    from trl import SFTTrainer
    from transformers import TrainingArguments
    from unsloth import is_bfloat16_supported
    from datasets import Dataset
    from huggingface_hub import HfApi
except ImportError:
    sys.exit("âŒ ä¾èµ–å®‰è£…å¤±è´¥ï¼Œè¯·é‡å¯è¿è¡Œæ—¶ã€‚")

# åŠ è½½æ¨¡å‹
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
max_seq_length = 2048

print(f"\nğŸš€ åŠ è½½åŸºåº§æ¨¡å‹: {model_name}...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
)

# å‡†å¤‡ Alpaca æ ¼å¼
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input, output) + tokenizer.eos_token
        texts.append(text)
    return { "text" : texts, }

# åŠ è½½æ¸…æ´—åçš„æ•°æ®
dataset = Dataset.from_json(CLEAN_DATA_FILE)
dataset = dataset.map(formatting_prompts_func, batched = True)

# --- 4. è®­ç»ƒæ‰§è¡Œ (å‚æ•°å·²é’ˆå¯¹ 6000æ¡+ æ•°æ®ä¼˜åŒ–) ---
print("\nâš”ï¸ è®­ç»ƒå¼€å§‹...")

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 20,          # å¢åŠ é¢„çƒ­
        max_steps = 1500,           # å…³é”®è°ƒæ•´ï¼šæå‡åˆ° 1500 æ­¥ä»¥é€‚åº”å¤§é‡æ•°æ®
        learning_rate = 1e-4,       # å…³é”®è°ƒæ•´ï¼šé™ä½å­¦ä¹ ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 5,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "cosine", # ä½¿ç”¨ä½™å¼¦é€€ç«ç­–ç•¥
        seed = 3407,
        output_dir = "outputs",
        report_to = "none",
    ),
)

trainer.train()

# --- 5. äº¤ä»˜æµæ°´çº¿ ---
print("\n" + "="*50)
print("ğŸ è®­ç»ƒå®Œæˆï¼Œå¯åŠ¨è‡ªåŠ¨åŒ–äº¤ä»˜...")
print("="*50)

# ä¸Šä¼  LoRA
try:
    print(f"â˜ï¸ åŒæ­¥ LoRA è‡³ HF: {HF_REPO_NAME}...")
    model.push_to_hub(HF_REPO_NAME, token = HF_TOKEN)
    tokenizer.push_to_hub(HF_REPO_NAME, token = HF_TOKEN)
except Exception as e:
    print(f"âš ï¸ LoRA ä¸Šä¼ è­¦å‘Š: {e}")

# è½¬æ¢å¹¶ä¸Šä¼  GGUF
print(f"\nğŸ“¦ æ‰§è¡Œ GGUF è½¬æ¢ ({QUANTIZATION_METHOD})...")
clean_memory()
try:
    output_dir = "model_gguf"
    model.save_pretrained_gguf(output_dir, tokenizer, quantization_method = QUANTIZATION_METHOD)
    
    gguf_files = [f for f in os.listdir(output_dir) if f.endswith(".gguf")]
    if gguf_files:
        local_gguf_path = os.path.join(output_dir, gguf_files[0])
        print(f"ğŸ¯ æ–‡ä»¶ç”Ÿæˆ: {local_gguf_path}")
        
        print(f"ğŸš€ ä¸Šä¼ å¤§æ–‡ä»¶åˆ° Hugging Face...")
        api = HfApi()
        api.upload_file(
            path_or_fileobj=local_gguf_path,
            path_in_repo=gguf_files[0],
            repo_id=HF_REPO_NAME,
            token=HF_TOKEN
        )
        print("âœ… GGUF ä¸Šä¼ æˆåŠŸï¼")
        print(f"ğŸ”— ä¸‹è½½åœ°å€: https://huggingface.co/{HF_REPO_NAME}/tree/main")
    else:
        print("âŒ æœªç”Ÿæˆ GGUF æ–‡ä»¶ã€‚")
except Exception as e:
    print(f"âŒ äº¤ä»˜é”™è¯¯: {e}")

print("\nğŸ‰ å¤§å¸ˆçº§è®­ç»ƒæµç¨‹ç»“æŸã€‚")