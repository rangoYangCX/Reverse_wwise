# -*- coding: utf-8 -*-
"""
ã€æ•°æ®é›†åˆ†æä¸é¢„å¤„ç†å·¥å…·ã€‘V1.0
åŠŸèƒ½ï¼š
1. åˆ†ææ•°æ®é›†çš„æ ·æœ¬åˆ†å¸ƒï¼ˆç±»å‹ã€é•¿åº¦ã€å¤æ‚åº¦ï¼‰
2. æ£€æŸ¥æ˜¯å¦åŒ…å«å„ç±»æ•°æ®ï¼ˆAudio/Event/å‚æ•°ï¼‰
3. è‡ªåŠ¨è®¡ç®—æœ€ä½³ max_seq_length
4. è¿‡æ»¤æˆ–æˆªæ–­è¶…é•¿æ ·æœ¬
5. ç”Ÿæˆè®­ç»ƒå°±ç»ªçš„æ•°æ®é›†

ä½¿ç”¨æ–¹æ³•ï¼š
    python dataset_analyzer.py optimized_dataset_processed.jsonl
"""

import json
import argparse
from collections import Counter, defaultdict
from typing import List, Dict, Tuple
import os

# =============================================================================
# æ•°æ®é›†åˆ†æå™¨
# =============================================================================

class DatasetAnalyzer:
    """æ•°æ®é›†åˆ†æå™¨"""
    
    def __init__(self, samples: List[Dict]):
        self.samples = samples
        self.stats = {}
    
    def analyze(self) -> Dict:
        """å®Œæ•´åˆ†æ"""
        print("=" * 60)
        print("ğŸ“Š æ•°æ®é›†åˆ†æ")
        print("=" * 60)
        
        # åŸºç¡€ç»Ÿè®¡
        self.stats["total_samples"] = len(self.samples)
        print(f"\næ€»æ ·æœ¬æ•°: {self.stats['total_samples']}")
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        self._analyze_by_type()
        
        # æŒ‰å¤æ‚åº¦ç»Ÿè®¡
        self._analyze_by_complexity()
        
        # é•¿åº¦åˆ†æ
        self._analyze_length()
        
        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        self._check_data_coverage()
        
        # Token ä¼°ç®—
        self._estimate_tokens()
        
        return self.stats
    
    def _analyze_by_type(self):
        """æŒ‰ root_type åˆ†ç±»ç»Ÿè®¡"""
        type_counter = Counter()
        for s in self.samples:
            root_type = s.get("meta", {}).get("root_type", "Unknown")
            type_counter[root_type] += 1
        
        self.stats["by_type"] = dict(type_counter)
        
        print(f"\nğŸ“‚ æŒ‰ç±»å‹åˆ†å¸ƒ:")
        for t, count in type_counter.most_common():
            pct = count / len(self.samples) * 100
            print(f"   {t}: {count} ({pct:.1f}%)")
    
    def _analyze_by_complexity(self):
        """æŒ‰å¤æ‚åº¦ç»Ÿè®¡"""
        complexity_counter = Counter()
        for s in self.samples:
            complexity = s.get("meta", {}).get("complexity", "Unknown")
            complexity_counter[complexity] += 1
        
        self.stats["by_complexity"] = dict(complexity_counter)
        
        print(f"\nğŸ“ˆ æŒ‰å¤æ‚åº¦åˆ†å¸ƒ:")
        for c, count in complexity_counter.most_common():
            pct = count / len(self.samples) * 100
            print(f"   {c}: {count} ({pct:.1f}%)")
    
    def _analyze_length(self):
        """é•¿åº¦åˆ†æ"""
        line_counts = []
        char_counts = []
        
        for s in self.samples:
            output = s.get("output", "")
            line_counts.append(s.get("meta", {}).get("line_count", output.count("\n") + 1))
            char_counts.append(len(output))
        
        self.stats["line_count"] = {
            "min": min(line_counts),
            "max": max(line_counts),
            "avg": sum(line_counts) / len(line_counts),
            "median": sorted(line_counts)[len(line_counts) // 2]
        }
        
        self.stats["char_count"] = {
            "min": min(char_counts),
            "max": max(char_counts),
            "avg": sum(char_counts) / len(char_counts),
            "median": sorted(char_counts)[len(char_counts) // 2]
        }
        
        # é•¿åº¦åˆ†å¸ƒ
        length_buckets = {
            "1-30è¡Œ": 0,
            "31-50è¡Œ": 0,
            "51-100è¡Œ": 0,
            "101-150è¡Œ": 0,
            "151-200è¡Œ": 0,
            "200+è¡Œ": 0
        }
        
        for lc in line_counts:
            if lc <= 30:
                length_buckets["1-30è¡Œ"] += 1
            elif lc <= 50:
                length_buckets["31-50è¡Œ"] += 1
            elif lc <= 100:
                length_buckets["51-100è¡Œ"] += 1
            elif lc <= 150:
                length_buckets["101-150è¡Œ"] += 1
            elif lc <= 200:
                length_buckets["151-200è¡Œ"] += 1
            else:
                length_buckets["200+è¡Œ"] += 1
        
        self.stats["length_distribution"] = length_buckets
        
        print(f"\nğŸ“ é•¿åº¦ç»Ÿè®¡:")
        print(f"   è¡Œæ•°: æœ€å°={self.stats['line_count']['min']}, æœ€å¤§={self.stats['line_count']['max']}, å¹³å‡={self.stats['line_count']['avg']:.1f}, ä¸­ä½æ•°={self.stats['line_count']['median']}")
        print(f"   å­—ç¬¦: æœ€å°={self.stats['char_count']['min']}, æœ€å¤§={self.stats['char_count']['max']}, å¹³å‡={self.stats['char_count']['avg']:.0f}")
        
        print(f"\nğŸ“Š é•¿åº¦åˆ†å¸ƒ:")
        for bucket, count in length_buckets.items():
            pct = count / len(self.samples) * 100
            bar = "â–ˆ" * int(pct / 2)
            print(f"   {bucket}: {count:5d} ({pct:5.1f}%) {bar}")
    
    def _check_data_coverage(self):
        """æ£€æŸ¥æ•°æ®è¦†ç›–å®Œæ•´æ€§"""
        coverage = {
            "has_audio": False,      # Container/Sound å±‚çº§
            "has_event": False,      # Event + ADD_ACTION
            "has_attenuation": False, # Attenuation æ›²çº¿
            "has_game_param": False,  # GameParameter
            "has_switch_group": False, # SwitchGroup
            "has_state_group": False,  # StateGroup
            "has_workflow": False,    # Event + Target ç»„åˆ
        }
        
        for s in self.samples:
            root_type = s.get("meta", {}).get("root_type", "")
            output = s.get("output", "")
            
            if root_type in ["RandomSequenceContainer", "SwitchContainer", "BlendContainer", "ActorMixer"]:
                coverage["has_audio"] = True
            
            if root_type == "Event":
                coverage["has_event"] = True
                # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´å·¥ä½œæµï¼ˆEvent + åŒ…å« Container åˆ›å»ºï¼‰
                if "RandomSequenceContainer" in output or "SwitchContainer" in output:
                    coverage["has_workflow"] = True
            
            # æ£€æŸ¥ Workflow ç±»å‹ï¼ˆç”± dataset_optimizer ç”Ÿæˆï¼‰
            if root_type == "Workflow":
                coverage["has_event"] = True
                coverage["has_workflow"] = True
            
            if root_type == "Attenuation":
                coverage["has_attenuation"] = True
            
            if root_type == "GameParameter":
                coverage["has_game_param"] = True
            
            if root_type == "SwitchGroup":
                coverage["has_switch_group"] = True
            
            if root_type == "StateGroup":
                coverage["has_state_group"] = True
        
        self.stats["coverage"] = coverage
        
        print(f"\nâœ… æ•°æ®è¦†ç›–æ£€æŸ¥:")
        status = {True: "âœ“", False: "âœ—"}
        print(f"   {status[coverage['has_audio']]} Audio å±‚çº§ (Container/Sound)")
        print(f"   {status[coverage['has_event']]} Event äº‹ä»¶")
        print(f"   {status[coverage['has_attenuation']]} Attenuation æ›²çº¿")
        print(f"   {status[coverage['has_game_param']]} GameParameter å‚æ•°")
        print(f"   {status[coverage['has_switch_group']]} SwitchGroup åˆ‡æ¢ç»„")
        print(f"   {status[coverage['has_state_group']]} StateGroup çŠ¶æ€ç»„")
        print(f"   {status[coverage['has_workflow']]} Event+Target å·¥ä½œæµ")
    
    def _estimate_tokens(self):
        """ä¼°ç®— Token æ•°é‡"""
        # ç²—ç•¥ä¼°ç®—: ä¸­æ–‡çº¦ 1.5 token/å­—ç¬¦, è‹±æ–‡çº¦ 0.25 token/å­—ç¬¦
        # DSL ä»£ç ä¸»è¦æ˜¯è‹±æ–‡ï¼Œä¼°ç®— 0.3 token/å­—ç¬¦
        
        token_estimates = []
        for s in self.samples:
            instruction = s.get("instruction", "")
            output = s.get("output", "")
            
            # å®Œæ•´ prompt çš„å­—ç¬¦æ•°
            total_chars = len(instruction) + len(output) + 100  # 100 for system prompt overhead
            
            # ä¼°ç®— tokens (DSLä»£ç ä¸»è¦æ˜¯è‹±æ–‡å…³é”®è¯)
            estimated_tokens = int(total_chars * 0.35)
            token_estimates.append(estimated_tokens)
        
        self.stats["token_estimate"] = {
            "min": min(token_estimates),
            "max": max(token_estimates),
            "avg": sum(token_estimates) / len(token_estimates),
            "p90": sorted(token_estimates)[int(len(token_estimates) * 0.9)],
            "p95": sorted(token_estimates)[int(len(token_estimates) * 0.95)],
            "p99": sorted(token_estimates)[int(len(token_estimates) * 0.99)],
        }
        
        # è¶…é•¿æ ·æœ¬ç»Ÿè®¡
        over_2048 = len([t for t in token_estimates if t > 2048])
        over_4096 = len([t for t in token_estimates if t > 4096])
        
        self.stats["overlength"] = {
            "over_2048": over_2048,
            "over_4096": over_4096,
        }
        
        print(f"\nğŸ”¢ Token ä¼°ç®— (è¿‘ä¼¼å€¼):")
        print(f"   æœ€å°: {self.stats['token_estimate']['min']}")
        print(f"   æœ€å¤§: {self.stats['token_estimate']['max']}")
        print(f"   å¹³å‡: {self.stats['token_estimate']['avg']:.0f}")
        print(f"   P90: {self.stats['token_estimate']['p90']}")
        print(f"   P95: {self.stats['token_estimate']['p95']}")
        print(f"   P99: {self.stats['token_estimate']['p99']}")
        
        print(f"\nâš ï¸ è¶…é•¿æ ·æœ¬:")
        print(f"   è¶…è¿‡ 2048 tokens: {over_2048} ({over_2048/len(self.samples)*100:.1f}%)")
        print(f"   è¶…è¿‡ 4096 tokens: {over_4096} ({over_4096/len(self.samples)*100:.1f}%)")
        
        # æ¨è max_seq_length
        if self.stats['token_estimate']['p95'] <= 2048:
            recommended = 2048
        elif self.stats['token_estimate']['p95'] <= 4096:
            recommended = 4096
        else:
            recommended = 8192
        
        self.stats["recommended_max_seq_length"] = recommended
        print(f"\nğŸ’¡ æ¨è max_seq_length: {recommended}")


# =============================================================================
# æ•°æ®é›†é¢„å¤„ç†å™¨
# =============================================================================

class DatasetPreprocessor:
    """æ•°æ®é›†é¢„å¤„ç†å™¨"""
    
    def __init__(self, samples: List[Dict]):
        self.samples = samples
        self.processed = []
    
    def process(
        self,
        max_lines: int = 100,
        max_tokens: int = 2048,
        strategy: str = "truncate",  # truncate, filter, split
        keep_ratio: float = 0.95,     # ä¿ç•™ 95% çš„æ ·æœ¬
    ) -> List[Dict]:
        """
        é¢„å¤„ç†æ•°æ®é›†
        
        Args:
            max_lines: æœ€å¤§è¡Œæ•°
            max_tokens: æœ€å¤§ token æ•°ï¼ˆä¼°ç®—ï¼‰
            strategy: å¤„ç†ç­–ç•¥
                - truncate: æˆªæ–­è¶…é•¿éƒ¨åˆ†
                - filter: è¿‡æ»¤è¶…é•¿æ ·æœ¬
                - split: æ‹†åˆ†è¶…é•¿æ ·æœ¬ï¼ˆæš‚æœªå®ç°ï¼‰
            keep_ratio: æœŸæœ›ä¿ç•™çš„æ ·æœ¬æ¯”ä¾‹
        """
        print("\n" + "=" * 60)
        print("ğŸ”§ æ•°æ®é›†é¢„å¤„ç†")
        print("=" * 60)
        print(f"   ç­–ç•¥: {strategy}")
        print(f"   æœ€å¤§è¡Œæ•°: {max_lines}")
        print(f"   æœ€å¤§ tokens (ä¼°ç®—): {max_tokens}")
        
        self.processed = []
        truncated_count = 0
        filtered_count = 0
        
        for s in self.samples:
            output = s.get("output", "")
            lines = output.split("\n")
            line_count = len(lines)
            
            # ä¼°ç®— token æ•°
            total_chars = len(s.get("instruction", "")) + len(output) + 100
            estimated_tokens = int(total_chars * 0.35)
            
            # åˆ¤æ–­æ˜¯å¦è¶…é•¿
            is_overlength = line_count > max_lines or estimated_tokens > max_tokens
            
            if not is_overlength:
                # æ­£å¸¸æ ·æœ¬ç›´æ¥æ·»åŠ 
                self.processed.append(s)
            elif strategy == "filter":
                # è¿‡æ»¤ç­–ç•¥ï¼šç›´æ¥è·³è¿‡
                filtered_count += 1
            elif strategy == "truncate":
                # æˆªæ–­ç­–ç•¥
                if line_count > max_lines:
                    # ä¿ç•™å‰ max_lines è¡Œ
                    truncated_output = "\n".join(lines[:max_lines])
                    new_sample = s.copy()
                    new_sample["output"] = truncated_output
                    new_sample["meta"] = s.get("meta", {}).copy()
                    new_sample["meta"]["line_count"] = max_lines
                    new_sample["meta"]["truncated"] = True
                    new_sample["meta"]["original_line_count"] = line_count
                    self.processed.append(new_sample)
                    truncated_count += 1
                else:
                    self.processed.append(s)
        
        print(f"\nğŸ“Š å¤„ç†ç»“æœ:")
        print(f"   åŸå§‹æ ·æœ¬: {len(self.samples)}")
        print(f"   å¤„ç†å: {len(self.processed)}")
        if strategy == "truncate":
            print(f"   æˆªæ–­æ ·æœ¬: {truncated_count}")
        elif strategy == "filter":
            print(f"   è¿‡æ»¤æ ·æœ¬: {filtered_count}")
        
        actual_ratio = len(self.processed) / len(self.samples)
        print(f"   ä¿ç•™æ¯”ä¾‹: {actual_ratio*100:.1f}%")
        
        return self.processed
    
    def balance_dataset(self, target_ratio: Dict[str, float] = None) -> List[Dict]:
        """
        å¹³è¡¡æ•°æ®é›†ï¼ˆæŒ‰ç±»å‹ï¼‰
        
        Args:
            target_ratio: ç›®æ ‡æ¯”ä¾‹ï¼Œå¦‚ {"audio": 0.5, "event": 0.2, ...}
        """
        if target_ratio is None:
            # é»˜è®¤æ¯”ä¾‹
            target_ratio = {
                "audio": 0.50,      # Container/Sound
                "event": 0.20,      # Event
                "attenuation": 0.12,
                "gameparam": 0.10,
                "switch_state": 0.08,
            }
        
        # åˆ†ç±»æ ·æœ¬
        categorized = defaultdict(list)
        for s in self.processed:
            root_type = s.get("meta", {}).get("root_type", "")
            
            if root_type in ["RandomSequenceContainer", "SwitchContainer", "BlendContainer", "ActorMixer"]:
                categorized["audio"].append(s)
            elif root_type == "Event":
                categorized["event"].append(s)
            elif root_type == "Attenuation":
                categorized["attenuation"].append(s)
            elif root_type == "GameParameter":
                categorized["gameparam"].append(s)
            elif root_type in ["SwitchGroup", "StateGroup"]:
                categorized["switch_state"].append(s)
            else:
                categorized["other"].append(s)
        
        print(f"\nğŸ“Š å½“å‰åˆ†å¸ƒ:")
        for cat, samples in categorized.items():
            print(f"   {cat}: {len(samples)}")
        
        return self.processed
    
    def save(self, output_path: str):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®é›†"""
        with open(output_path, 'w', encoding='utf-8') as f:
            for s in self.processed:
                f.write(json.dumps(s, ensure_ascii=False) + '\n')
        
        print(f"\nâœ… å·²ä¿å­˜åˆ°: {output_path}")
        print(f"   æ ·æœ¬æ•°: {len(self.processed)}")


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def load_jsonl(path: str) -> List[Dict]:
    """åŠ è½½ JSONL æ–‡ä»¶"""
    samples = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                samples.append(json.loads(line))
    return samples


def main():
    parser = argparse.ArgumentParser(description="æ•°æ®é›†åˆ†æä¸é¢„å¤„ç†å·¥å…·")
    parser.add_argument("input", type=str, help="è¾“å…¥ JSONL æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", type=str, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max-lines", type=int, default=100, help="æœ€å¤§è¡Œæ•° (é»˜è®¤ 100)")
    parser.add_argument("--max-tokens", type=int, default=2048, help="æœ€å¤§ tokens (é»˜è®¤ 2048)")
    parser.add_argument("--strategy", type=str, default="truncate", 
                        choices=["truncate", "filter"], help="å¤„ç†ç­–ç•¥")
    parser.add_argument("--analyze-only", action="store_true", help="ä»…åˆ†æï¼Œä¸å¤„ç†")
    
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {args.input}")
    samples = load_jsonl(args.input)
    
    # åˆ†æ
    analyzer = DatasetAnalyzer(samples)
    stats = analyzer.analyze()
    
    if args.analyze_only:
        print("\nâœ… åˆ†æå®Œæˆ (ä»…åˆ†ææ¨¡å¼)")
        return
    
    # é¢„å¤„ç†
    preprocessor = DatasetPreprocessor(samples)
    processed = preprocessor.process(
        max_lines=args.max_lines,
        max_tokens=args.max_tokens,
        strategy=args.strategy
    )
    
    # ä¿å­˜
    if args.output:
        output_path = args.output
    else:
        base, ext = os.path.splitext(args.input)
        output_path = f"{base}_processed{ext}"
    
    preprocessor.save(output_path)
    
    # è¾“å‡ºæ¨èé…ç½®
    print("\n" + "=" * 60)
    print("ğŸ’¡ æ¨è Colab è®­ç»ƒé…ç½®")
    print("=" * 60)
    print(f"""
# åŸºäºæ•°æ®åˆ†æçš„æ¨èé…ç½®
MAX_SEQ_LENGTH = {stats['recommended_max_seq_length']}

# æ•°æ®é›†ä¿¡æ¯
DATASET_PATH = "{output_path}"
TOTAL_SAMPLES = {len(processed)}

# è®­ç»ƒå‚æ•° (æ ¹æ®æ ·æœ¬é‡è°ƒæ•´)
TRAINING_CONFIG = {{
    "num_epochs": 3,
    "learning_rate": 2e-4,
    "batch_size": 4,
    "gradient_accumulation": 4,
}}
""")


if __name__ == "__main__":
    main()
