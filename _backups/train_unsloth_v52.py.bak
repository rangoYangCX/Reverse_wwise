# =============================================================================
# ğŸš€ Wwise Engineering Brain - ç»ˆæå…¨è‡ªåŠ¨è®­ç»ƒè„šæœ¬ V5.2 (æ˜¾å­˜ä¼˜åŒ–ç‰ˆ)
# =============================================================================
# æ ¸å¿ƒä¼˜åŒ–:
# 1. [æ™ºèƒ½] è‡ªåŠ¨è¯†åˆ« L4/A100 é«˜æ˜¾å­˜ç¯å¢ƒï¼Œè‡ªåŠ¨å¼€å¯ä¼˜åŒ– Batch æ¨¡å¼
# 2. [æé€Ÿ] è®­ç»ƒæ—¶é•¿æ§åˆ¶åœ¨ 3-4 å°æ—¶ (L4 GPU)
# 3. [åŒæº] ä¼˜å…ˆè¯»å–æœ¬åœ°ä¸Šä¼ çš„ JSONLï¼Œæ— æ–‡ä»¶åˆ™è‡ªåŠ¨ä» HF ä¸‹è½½
# 4. [å®‰å…¨] æ”¯æŒ Colab Secrets æˆ–æ‰‹åŠ¨è¾“å…¥ Token
# 5. [ç¨³å¥] OOM è‡ªåŠ¨é™çº§ + æ·±åº¦æ˜¾å­˜æ¸…ç†ï¼Œé˜²æ­¢å´©æºƒ
# 6. [å®Œæ•´] åŒ…å«è®­ç»ƒã€LoRAå¤‡ä»½ã€GGUFè½¬æ¢ã€Modelfileç”Ÿæˆã€è‡ªåŠ¨ä¸Šä¼ 
# =============================================================================

import os
import sys
import subprocess
import json
import torch
import gc
import math
import time
from datetime import timedelta

# --- 0. ç¯å¢ƒä¸ä¾èµ–è‡ªæ£€ (æœ€ä¼˜å…ˆæ‰§è¡Œ) ---
print("="*60)
print("ğŸ”§ ç¯å¢ƒåˆå§‹åŒ–...")
print("="*60)

# ä¿®å¤ Unsloth psutil ä¾èµ–é—®é¢˜
try:
    import psutil
    import builtins
    builtins.psutil = psutil
except ImportError:
    subprocess.check_call("pip install psutil", shell=True)
    import psutil
    import builtins
    builtins.psutil = psutil

def install_package(pkg):
    try:
        subprocess.check_call(f"pip install {pkg}", shell=True)
    except: pass

print("ğŸ“¦ å®‰è£…æ ¸å¿ƒä¾èµ– (Unsloth & HF)...")
install_package("unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git")
install_package("--no-deps xformers trl peft accelerate bitsandbytes huggingface_hub datasets")

from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from huggingface_hub import HfApi, hf_hub_download, create_repo
from google.colab import userdata
from datasets import Dataset

# =============================================================================
# âš™ï¸ ç”¨æˆ·é…ç½®åŒº
# =============================================================================

# 1. é‰´æƒé…ç½®
try:
    HF_TOKEN = userdata.get('HF_TOKEN')
    print("âœ… ä» Colab Secrets è¯»å– Token æˆåŠŸ")
except:
    print("âš ï¸ æœªæ‰¾åˆ° Colab Secret")
    print("   æ–¹æ³•1: ç‚¹å‡»å·¦ä¾§ ğŸ”‘ å›¾æ ‡ â†’ æ·»åŠ  HF_TOKEN")
    print("   æ–¹æ³•2: æ‰‹åŠ¨è¾“å…¥ Token")
    HF_TOKEN = input("\nè¯·è¾“å…¥ HuggingFace Token (ç•™ç©ºé€€å‡º): ").strip()
    if not HF_TOKEN:
        print("âŒ Token ä¸èƒ½ä¸ºç©ºï¼Œé€€å‡º")
        raise SystemExit()

os.environ["HF_TOKEN"] = HF_TOKEN

# 2. ä»“åº“é…ç½®
HF_USER = "chengxuanyyy"
REPO_NAME = "Wwise-Engineering-Brain"
HF_MODEL_REPO = f"{HF_USER}/{REPO_NAME}"

# 3. æ•°æ®é›†é…ç½®
LOCAL_DATASET_NAME = "wwise_phase2_full_22k.jsonl" 
HF_DATASET_REPO = "chengxuanyyy/wwise-dsl-training-data"
HF_DATASET_FILENAME = "optimized_dataset_processed_processed.jsonl"

# 4. æ¨¡å‹ä¸è®­ç»ƒå‚æ•°
BASE_MODEL = "Qwen/Qwen2.5-Coder-7B-Instruct"
MAX_SEQ_LENGTH = 2048 
QUANTIZATION_METHOD = "q4_k_m"
TARGET_EPOCHS = 2

# =============================================================================
# ğŸï¸ æ˜¾å­˜æ™ºèƒ½æ£€æµ‹ä¸ Batch Size ä¼˜åŒ–
# =============================================================================
print("\n" + "="*60)
print("ğŸï¸ ç¡¬ä»¶æ€§èƒ½è¯„ä¼°")
print("="*60)

gpu_name = torch.cuda.get_device_name(0)
total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
print(f"ğŸ® æ£€æµ‹åˆ° GPU: {gpu_name} ({total_vram:.2f} GB VRAM)")

# åŠ¨æ€è®¾å®š Batch Size (å®‰å…¨ä¼˜åŒ–ç‰ˆ)
if total_vram > 35:  # A100 (40GB)
    BATCH_SIZE_PER_DEVICE = 8
    GRAD_ACCUM_STEPS = 4
    print("ğŸš€ A100 ç¯å¢ƒ: æ¿€æ´»æé€Ÿæ¨¡å¼ (Batch Size = 8)")
elif total_vram > 20:  # L4 (24GB)
    BATCH_SIZE_PER_DEVICE = 4  # å®‰å…¨å€¼ï¼Œé¿å… OOM
    GRAD_ACCUM_STEPS = 4
    print("ğŸš€ L4 ç¯å¢ƒ: Turbo æ¨¡å¼ (Batch Size = 4)")
elif total_vram > 14:  # T4 (16GB)
    BATCH_SIZE_PER_DEVICE = 2
    GRAD_ACCUM_STEPS = 4
    print("ğŸ›¡ï¸ T4 ç¯å¢ƒ: å®‰å…¨æ¨¡å¼ (Batch Size = 2)")
else:
    BATCH_SIZE_PER_DEVICE = 1
    GRAD_ACCUM_STEPS = 8
    print("âš ï¸ ä½æ˜¾å­˜ç¯å¢ƒ: ä¿å®ˆæ¨¡å¼")

EFFECTIVE_BATCH_SIZE = BATCH_SIZE_PER_DEVICE * GRAD_ACCUM_STEPS
print(f"âš¡ æ€»æœ‰æ•ˆæ‰¹æ¬¡ (Effective Batch Size): {EFFECTIVE_BATCH_SIZE}")

# =============================================================================
# ğŸ“Š Step 1: æ™ºèƒ½æ•°æ®åŠ è½½ä¸åˆ†æ
# =============================================================================
print("\n" + "="*60)
print("ğŸ“‚ Step 1: æ•°æ®åŠ è½½ä¸åˆ†æ")
print("="*60)

data_file_path = ""

if os.path.exists(LOCAL_DATASET_NAME):
    print(f"âœ… å‘ç°æœ¬åœ°æ•°æ®é›†: {LOCAL_DATASET_NAME}")
    data_file_path = LOCAL_DATASET_NAME
else:
    print(f"âš ï¸ æœ¬åœ°æœªæ‰¾åˆ°ï¼Œä» HuggingFace ä¸‹è½½...")
    try:
        data_file_path = hf_hub_download(
            repo_id=HF_DATASET_REPO,
            filename=HF_DATASET_FILENAME,
            repo_type="dataset",
            token=HF_TOKEN
        )
        print(f"âœ… ä¸‹è½½æˆåŠŸ")
    except Exception as e:
        sys.exit(f"âŒ æ•°æ®è·å–å¤±è´¥: {e}")

# è¯»å–å¹¶ç»Ÿè®¡
samples = []
with open(data_file_path, 'r', encoding='utf-8', errors='ignore') as f:
    for line in f:
        if line.strip():
            try:
                samples.append(json.loads(line))
            except: pass

TOTAL_SAMPLES = len(samples)
print(f"ğŸ“Š æœ‰æ•ˆæ ·æœ¬æ•°: {TOTAL_SAMPLES}")

# åŠ¨æ€æ­¥æ•°è®¡ç®—
STEPS_PER_EPOCH = math.ceil(TOTAL_SAMPLES / EFFECTIVE_BATCH_SIZE)
MAX_STEPS = int(STEPS_PER_EPOCH * TARGET_EPOCHS)

print(f"ğŸ§® è®¡ç®—å‚æ•°:")
print(f"   - Effective Batch Size: {EFFECTIVE_BATCH_SIZE}")
print(f"   - Steps per Epoch: {STEPS_PER_EPOCH}")
print(f"   - Target Epochs: {TARGET_EPOCHS}")
print(f"ğŸ¯ æœ€ç»ˆè®­ç»ƒæ­¥æ•°: {MAX_STEPS} æ­¥")

# é¢„ä¼°æ—¶é—´
if "L4" in gpu_name:
    estimated_hours = MAX_STEPS * 4 / 3600  # L4 çº¦ 4ç§’/æ­¥
elif "A100" in gpu_name:
    estimated_hours = MAX_STEPS * 2 / 3600  # A100 çº¦ 2ç§’/æ­¥
else:
    estimated_hours = MAX_STEPS * 8 / 3600  # T4 çº¦ 8ç§’/æ­¥
print(f"â±ï¸ é¢„ä¼°è®­ç»ƒæ—¶é—´: {estimated_hours:.1f} å°æ—¶")

# =============================================================================
# ğŸ¤– Step 2: åŠ è½½æ¨¡å‹
# =============================================================================
print("\n" + "="*60)
print("ğŸ¤– Step 2: åŠ è½½æ¨¡å‹ (Unsloth Accelerated)")
print("="*60)

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=BASE_MODEL,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,
    load_in_4bit=True,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=64,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=128,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# =============================================================================
# ğŸ“ Step 3: æ•°æ®æ ¼å¼åŒ–
# =============================================================================
print("\n" + "="*60)
print("ğŸ“ Step 3: æ•°æ®é›†æ ¼å¼åŒ–")
print("="*60)

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]
    texts = []
    for instruction, input_text, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input_text, output) + tokenizer.eos_token
        texts.append(text)
    return {"text": texts}

dataset = Dataset.from_list(samples)
dataset = dataset.map(formatting_prompts_func, batched=True)
print(f"âœ… æ ¼å¼åŒ–å®Œæˆ")

# =============================================================================
# âš”ï¸ Step 4: å¼€å§‹è®­ç»ƒ
# =============================================================================
print("\n" + "="*60)
print("âš”ï¸ Step 4: å¼€å§‹è®­ç»ƒ")
print("="*60)

# è®­ç»ƒå‰æ˜¾å­˜æ¸…ç†
gc.collect()
torch.cuda.empty_cache()
free_mem = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3
print(f"ğŸ“Š å½“å‰å¯ç”¨æ˜¾å­˜: {free_mem:.2f} GB")

# OOM è‡ªåŠ¨é™çº§æœºåˆ¶
def try_train(batch_size, grad_accum):
    """å°è¯•è®­ç»ƒï¼ŒOOM æ—¶è¿”å› False"""
    global trainer
    try:
        training_args = TrainingArguments(
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=grad_accum,
            warmup_steps=50,
            max_steps=MAX_STEPS,
            learning_rate=1e-4,
            fp16=not torch.cuda.is_bf16_supported(),
            bf16=torch.cuda.is_bf16_supported(),
            logging_steps=20,
            optim="adamw_8bit",
            weight_decay=0.01,
            lr_scheduler_type="cosine",
            seed=3407,
            output_dir="outputs",
            report_to="none",
            save_strategy="steps",
            save_steps=500,
            save_total_limit=2,
        )
        training_args.dataset_num_proc = 2

        trainer = SFTTrainer(
            model=model,
            tokenizer=tokenizer,
            train_dataset=dataset,
            dataset_text_field="text",
            max_seq_length=MAX_SEQ_LENGTH,
            dataset_num_proc=2,
            packing=False,
            args=training_args,
        )
        
        trainer.train()
        return True
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print(f"\nâš ï¸ OOM! Batch Size {batch_size} å¤ªå¤§")
            del trainer
            gc.collect()
            torch.cuda.empty_cache()
            return False
        raise e

# å°è¯•è®­ç»ƒï¼ŒOOM æ—¶è‡ªåŠ¨é™çº§
start_time = time.time()
train_success = False

# é™çº§é¡ºåº: åŸå§‹ -> å‡åŠ -> æœ€å°
batch_configs = [
    (BATCH_SIZE_PER_DEVICE, GRAD_ACCUM_STEPS),
    (max(1, BATCH_SIZE_PER_DEVICE // 2), GRAD_ACCUM_STEPS * 2),
    (1, 8),
]

for bs, ga in batch_configs:
    print(f"\nğŸ¯ å°è¯•: Batch Size = {bs}, Grad Accum = {ga}")
    if try_train(bs, ga):
        train_success = True
        BATCH_SIZE_PER_DEVICE = bs  # æ›´æ–°å®é™…ä½¿ç”¨çš„å€¼
        GRAD_ACCUM_STEPS = ga
        break
    print("   é‡è¯•ä¸­...")

if not train_success:
    print("âŒ æ‰€æœ‰é…ç½®éƒ½ OOMï¼Œè¯·ä½¿ç”¨æ›´å¤§æ˜¾å­˜çš„ GPU")
    sys.exit(1)

train_time = str(timedelta(seconds=int(time.time() - start_time)))
print(f"\nâœ… è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {train_time}")

# æ˜¾å­˜æ¸…ç†
print("ğŸ§¹ æ¸…ç†æ˜¾å­˜...")
del trainer
gc.collect()
torch.cuda.empty_cache()

# =============================================================================
# ğŸ’¾ Step 5: å¯¼å‡ºä¸äº¤ä»˜
# =============================================================================
print("\n" + "="*60)
print("ğŸ’¾ Step 5: æˆæœå¯¼å‡ºä¸ GGUF è½¬æ¢")
print("="*60)

# æ·±åº¦æ¸…ç†æ˜¾å­˜ (GGUF è½¬æ¢éœ€è¦å¤§é‡å†…å­˜)
print("ğŸ§¹ æ·±åº¦æ¸…ç†æ˜¾å­˜ (ä¸º GGUF è½¬æ¢è…¾å‡ºç©ºé—´)...")
del model
gc.collect()
torch.cuda.empty_cache()
time.sleep(3)  # ç­‰å¾…æ˜¾å­˜å®Œå…¨é‡Šæ”¾

free_mem = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3
print(f"ğŸ“Š æ¸…ç†åå¯ç”¨æ˜¾å­˜: {free_mem:.2f} GB")

# é‡æ–°åŠ è½½æ¨¡å‹ç”¨äºå¯¼å‡º
print("ğŸ”„ é‡æ–°åŠ è½½æ¨¡å‹...")
model, tokenizer = FastLanguageModel.from_pretrained(
    "outputs",  # ä»è®­ç»ƒ checkpoint åŠ è½½
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,
    load_in_4bit=True,
)

# 1. ä¸Šä¼  LoRA Adapter
print("â˜ï¸ åŒæ­¥ LoRA Adapter...")
try:
    model.push_to_hub(HF_MODEL_REPO, token=HF_TOKEN, commit_message=f"LoRA (Steps: {MAX_STEPS}, Samples: {TOTAL_SAMPLES})")
    tokenizer.push_to_hub(HF_MODEL_REPO, token=HF_TOKEN)
    print("   âœ“ LoRA ä¸Šä¼ æˆåŠŸ")
except Exception as e:
    print(f"   âš ï¸ LoRA ä¸Šä¼ è­¦å‘Š: {e}")

# 2. è½¬æ¢ GGUF
print(f"ğŸ“¦ æ‰§è¡Œ GGUF è½¬æ¢ ({QUANTIZATION_METHOD})...")
try:
    output_dir = "model_gguf"
    model.save_pretrained_gguf(output_dir, tokenizer, quantization_method=QUANTIZATION_METHOD)
    
    gguf_files = [f for f in os.listdir(output_dir) if f.endswith(".gguf")]
    
    if gguf_files:
        local_path = os.path.join(output_dir, gguf_files[0])
        remote_filename = f"wwise-brain-v2-{QUANTIZATION_METHOD}.gguf"
        
        print(f"ğŸš€ ä¸Šä¼ æ¨¡å‹: {remote_filename}")
        api = HfApi(token=HF_TOKEN)
        
        try:
            api.create_repo(repo_id=HF_MODEL_REPO, exist_ok=True)
        except: pass

        api.upload_file(
            path_or_fileobj=local_path,
            path_in_repo=f"gguf/{remote_filename}",
            repo_id=HF_MODEL_REPO,
            token=HF_TOKEN
        )
        print("   âœ“ GGUF ä¸Šä¼ æˆåŠŸ")
        
        # 3. ç”Ÿæˆ Modelfile
        print("ğŸ“ ç”Ÿæˆ Modelfile...")
        modelfile_content = f'''FROM ./{remote_filename}

TEMPLATE """<|im_start|>system
{{{{ .System }}}}<|im_end|>
<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
<|im_start|>assistant
"""

SYSTEM """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Wwise éŸ³é¢‘æŠ€æœ¯ä¸“å®¶ï¼Œç²¾é€š DSL ä»£ç ç”Ÿæˆã€‚"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
'''
        with open("Modelfile", "w", encoding="utf-8") as f:
            f.write(modelfile_content)
            
        api.upload_file(
            path_or_fileobj="Modelfile",
            path_in_repo="gguf/Modelfile",
            repo_id=HF_MODEL_REPO,
            token=HF_TOKEN
        )
        print("   âœ“ Modelfile ä¸Šä¼ æˆåŠŸ")
        
        # 4. ç”Ÿæˆ README
        print("ğŸ“„ æ›´æ–° README...")
        readme_content = f"""---
license: apache-2.0
base_model: {BASE_MODEL}
tags:
  - wwise
  - audio
  - game-dev
  - dsl
  - unsloth
---

# ğŸ§ Wwise Engineering Brain V2.0

ä¸“ä¸º **æ¸¸æˆéŸ³é¢‘å·¥ç¨‹** è®¾è®¡çš„å‚ç›´é¢†åŸŸå¤§æ¨¡å‹ã€‚

## ğŸ“Š è®­ç»ƒä¿¡æ¯

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| åŸºåº§æ¨¡å‹ | {BASE_MODEL} |
| è®­ç»ƒæ ·æœ¬ | {TOTAL_SAMPLES} |
| è®­ç»ƒæ­¥æ•° | {MAX_STEPS} |
| è®­ç»ƒæ—¶é—´ | {train_time} |
| LoRA Rank | 64 |
| é‡åŒ–æ ¼å¼ | {QUANTIZATION_METHOD} |

## ğŸš€ å¿«é€Ÿå¼€å§‹ (Ollama)

```bash
# ä¸‹è½½æ¨¡å‹
huggingface-cli download {HF_MODEL_REPO} gguf/{remote_filename} --local-dir ./
huggingface-cli download {HF_MODEL_REPO} gguf/Modelfile --local-dir ./

# åˆ›å»ºå¹¶è¿è¡Œ
ollama create wwise-brain -f Modelfile
ollama run wwise-brain
```

## âœ… èƒ½åŠ›èŒƒå›´

- åˆ›å»º Audio å±‚çº§ç»“æ„ (Container/Sound)
- é…ç½® Attenuation è¡°å‡æ›²çº¿
- è®¾ç½® GameParameter RTPC å‚æ•°
- åˆ›å»º SwitchGroup/StateGroup
- ç”Ÿæˆ Event åŠå®Œæ•´å·¥ä½œæµ

## ğŸ“ æ–‡ä»¶è¯´æ˜

- `gguf/{remote_filename}`: é‡åŒ–æ¨¡å‹ (~4.5GB)
- `gguf/Modelfile`: Ollama é…ç½®æ–‡ä»¶
- å…¶ä»–æ–‡ä»¶: LoRA adapter

---

*Trained with Unsloth + NeuroWwise Pipeline*
"""
        with open("README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)
        
        api.upload_file(
            path_or_fileobj="README.md",
            path_in_repo="README.md",
            repo_id=HF_MODEL_REPO,
            token=HF_TOKEN
        )
        print("   âœ“ README ä¸Šä¼ æˆåŠŸ")

    else:
        print("âŒ GGUF ç”Ÿæˆå¤±è´¥")

except Exception as e:
    print(f"âŒ å¯¼å‡ºæµç¨‹å‡ºé”™: {e}")

# =============================================================================
# ğŸ‰ å®Œæˆ
# =============================================================================
print("\n" + "="*60)
print("ğŸ‰ å…¨æµç¨‹å®Œæˆ!")
print("="*60)
print(f"""
ğŸ“Š è®­ç»ƒç»Ÿè®¡:
   æ ·æœ¬æ•°: {TOTAL_SAMPLES}
   è®­ç»ƒæ­¥æ•°: {MAX_STEPS}
   è®­ç»ƒæ—¶é—´: {train_time}

ğŸ“¦ æ¨¡å‹åœ°å€:
   https://huggingface.co/{HF_MODEL_REPO}

ğŸš€ æœ¬åœ°ä½¿ç”¨:
   huggingface-cli download {HF_MODEL_REPO} gguf/wwise-brain-v2-{QUANTIZATION_METHOD}.gguf --local-dir ./
   huggingface-cli download {HF_MODEL_REPO} gguf/Modelfile --local-dir ./
   ollama create wwise-brain -f Modelfile
   ollama run wwise-brain
""")
print("="*60)
